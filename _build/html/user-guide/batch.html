
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Running Jobs on Cirrus &#8212; Cirrus 1.1 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '1.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Singularity Containers" href="singularity.html" />
    <link rel="prev" title="Application Development Environment" href="development.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="running-jobs-on-cirrus">
<h1>Running Jobs on Cirrus<a class="headerlink" href="#running-jobs-on-cirrus" title="Permalink to this headline">¶</a></h1>
<p>The Cirrus facility uses PBSPro to schedule jobs.</p>
<p>Writing a submission script is typically the most convenient way to
submit your job to the job submission system. Example submission scripts
(with explanations) for the most common job types are provided below.</p>
<p>Interactive jobs are also available and can be particularly useful for
developing and debugging applications. More details are available below.</p>
<p><strong>Note:</strong> There are a number of different queues on Cirrus. If you do not specify a
queue your job will be submitted to the default <code class="docutils literal"><span class="pre">workq</span></code> which has limits
on the maximum job size and total amount of resource that can be used.
See below for more information on the different queues and their limits.</p>
<p>If you have any questions on how to run jobs on Cirrus do not hesitate
to contact the EPCC Helpdesk.</p>
<div class="section" id="using-pbs-pro">
<h2>Using PBS Pro<a class="headerlink" href="#using-pbs-pro" title="Permalink to this headline">¶</a></h2>
<p>You typically interact with PBS by (1) specifying PBS directives in job
submission scripts (see examples below) and (2) issuing PBS commands
from the login nodes.</p>
<p>There are three key commands used to interact with the PBS on the
command line:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">qsub</span></code></li>
<li><code class="docutils literal"><span class="pre">qstat</span></code></li>
<li><code class="docutils literal"><span class="pre">qdel</span></code></li>
</ul>
<p>Check the PBS <code class="docutils literal"><span class="pre">man</span></code> page for more advanced commands:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">man</span> <span class="n">pbs</span>
</pre></div>
</div>
<div class="section" id="the-qsub-command">
<h3>The qsub command<a class="headerlink" href="#the-qsub-command" title="Permalink to this headline">¶</a></h3>
<p>The qsub command submits a job to PBS:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">qsub</span> <span class="n">job_script</span><span class="o">.</span><span class="n">pbs</span>
</pre></div>
</div>
<p>This will submit your job script “job_script.pbs” to the job-queues.
See the sections below for details on how to write job scripts.</p>
<p><strong>Note:</strong> There are a number of different queues on Cirrus. If you do not specify a
queue your job will be submitted to the default <code class="docutils literal"><span class="pre">workq</span></code> which has limits
on the maximum job size and total amount of resource that can be used.
See below for more information on the different queues and their limits.</p>
</div>
<div class="section" id="the-qstat-command">
<h3>The qstat command<a class="headerlink" href="#the-qstat-command" title="Permalink to this headline">¶</a></h3>
<p>Use the command qstat to view the job queue. For example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">qstat</span>
</pre></div>
</div>
<p>will list all jobs on Cirrus.</p>
<p>You can view just your jobs by using:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>qstat -u $USER
</pre></div>
</div>
<p>The <code class="docutils literal"><span class="pre">-a</span></code> option to qstat provides the output in a more useful
format.</p>
<p>To see more information about a queued job, use:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>qstat -f $JOBID
</pre></div>
</div>
<p>This option may be useful when your job fails to enter a running state.
The output contains a PBS <code class="docutils literal"><span class="pre">comment</span></code> field which may explain why the job
failed to run.</p>
</div>
<div class="section" id="the-qdel-command">
<h3>The qdel command<a class="headerlink" href="#the-qdel-command" title="Permalink to this headline">¶</a></h3>
<p>Use this command to delete a job from Cirrus’s job queue. For example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>qdel $JOBID
</pre></div>
</div>
<p>will remove the job with ID <code class="docutils literal"><span class="pre">$JOBID</span></code> from the queue.</p>
</div>
</div>
<div class="section" id="queue-limits">
<h2>Queue Limits<a class="headerlink" href="#queue-limits" title="Permalink to this headline">¶</a></h2>
<p>Queues on Cirrus are designed to enable users to use the system flexibly while
retaining fair access for all.</p>
<p>There are two queues available to general users on Cirrus:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">workq</span></code> (default): This is the default queue that user jobs are submitted to
if the <code class="docutils literal"><span class="pre">-q</span></code> option to <code class="docutils literal"><span class="pre">qsub</span></code> is not specified. Jobs in this queue can have a
maximum walltime of 96 hours (4 days) and a maximum job size of 2520 cores (70
nodes). Each <strong>project</strong> can use a maximum of 2520 cores (70 nodes) summed across all
their running jobs at any one time.</li>
<li><code class="docutils literal"><span class="pre">large</span></code>: Specified by using <code class="docutils literal"><span class="pre">-q</span> <span class="pre">large</span></code> at submission time. There is no
upper limit on job size in this queue but there is a minimum job size of 2521
cores (71 nodes), a maximum walltime of 48 hours (2 days),
each <strong>user</strong> can have a maximum of 1 job running at any one time, and a maximum
of 4 jobs in the queue (including a running job).</li>
</ul>
<p>If you try to submit a job that asks for more than the maximum allowed wall
time or cores you will see an error similar to:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>[user@cirrus-login0 ~]$ qsub submit.pbs
qsub: Job violates queue and/or server resource limits
</pre></div>
</div>
</div>
<div class="section" id="output-from-pbs-jobs">
<h2>Output from PBS jobs<a class="headerlink" href="#output-from-pbs-jobs" title="Permalink to this headline">¶</a></h2>
<p>PBS produces standard output and standard error for each batch job can
be found in files <code class="docutils literal"><span class="pre">&lt;jobname&gt;.o&lt;Job</span> <span class="pre">ID&gt;</span></code> and <code class="docutils literal"><span class="pre">&lt;jobname&gt;.e&lt;Job</span> <span class="pre">ID&gt;</span></code>
respectively. These files appear in the job’s working directory once
your job has completed or its maximum allocated time to run (i.e. wall
time, see later sections) has ran out.</p>
</div>
<div class="section" id="running-parallel-jobs">
<h2>Running Parallel Jobs<a class="headerlink" href="#running-parallel-jobs" title="Permalink to this headline">¶</a></h2>
<p>This section describes how to write job submission scripts specifically
for different kinds of parallel jobs on Cirrus.</p>
<p>All parallel job submission scripts require (as a minimum) you to
specify three things:</p>
<ul class="simple">
<li>The number of nodes and cores per node you require via the
<code class="docutils literal"><span class="pre">-l</span> <span class="pre">select=[Nodes]:ncpus=72</span></code> option. <strong>Note ncpus should always be 72, regardless of how many cores you intend to employ.  This simply indicates that you want to reserve all cores on a node.</strong> Each node has 36 physical
cores (2x 18-core sockets) and hyper-threads are enabled (2 per core) giving
a maximum of 72 cores per node (most users will actually only use a maximum of
36 cores per node for best performance). For example, to select 4 nodes
(144 physical cores in total) you would use
<code class="docutils literal"><span class="pre">-l</span> <span class="pre">select=4:ncpus=72</span></code>.</li>
<li>The maximum length of time (i.e. walltime) you want the job to run
for via the <code class="docutils literal"><span class="pre">-l</span> <span class="pre">walltime=[hh:mm:ss]</span></code> option. To ensure the
minimum wait time for your job, you should specify a walltime as
short as possible for your job (i.e. if your job is going to run for
3 hours, do not specify 12 hours). On average, the longer the
walltime you specify, the longer you will queue for.</li>
<li>The project code that you want to charge the job to via the
<code class="docutils literal"><span class="pre">-A</span> <span class="pre">[project</span> <span class="pre">code]</span></code> option</li>
</ul>
<p>In addition to these mandatory specifications, there are many other
options you can provide to PBS. The following options may be useful:</p>
<ul class="simple">
<li>By default compute nodes are shared, meaning other jobs may be placed
alongside yours if your resource request (with -l select) leaves some
cores free. To guarantee exclusive node usage, use the option <code class="docutils literal"><span class="pre">-l</span> <span class="pre">place=excl</span></code>.</li>
<li>The name for your job is set using <code class="docutils literal"><span class="pre">-N</span> <span class="pre">My_job</span></code>. In the examples below
the name will be “My_job”, but you can replace “My_job” with any
name you want. The name will be used in various places. In particular
it will be used in the queue listing and to generate the name of your
output and/or error file(s). Note there is a limit on the size of the
name.</li>
<li><code class="docutils literal"><span class="pre">-q</span> <span class="pre">large</span></code> will specify that you want to submit your job to the <code class="docutils literal"><span class="pre">large</span></code>
queue for running larger jobs than are permitted in the standard queue.</li>
</ul>
<div class="section" id="exclusive-node-access">
<h3>Exclusive Node Access<a class="headerlink" href="#exclusive-node-access" title="Permalink to this headline">¶</a></h3>
<p>By default on Cirrus, jobs are scheduled to nodes in a non-exclusive way.
This means that, by default, you will likely end up sharing a node with
another user. This can lead to variable and/or poor performance as you
will be potentially be competing with other users for resources such as
CPU and memory.</p>
<p>If you are running parallel jobs on Cirrus <strong>we strongly recommend that
you specify exclusive placement</strong> to ensure that you get the best performance
out of the compute nodes. The only case where you may not want to use
exclusive placement for parallel jobs is when you are using a very small
number of cores (e.g. less than half a node, 18 cores). Even then, you
may find that the benefits of exclusive placement outweigh the addition
costs incurred (as you are charged for all the cores on a node in
exclusive mode).</p>
<p>To make sure your josb have exclusive node access you should add the
following PBS option to your jobs:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1">#PBS -l place=excl</span>
</pre></div>
</div>
<p>All of our example parallel job submission scripts below specify this option.</p>
<p>Of course, if you are running a serial job then you should not generally
specify this option as it would result in you reserving (and being charged for)
a full 36 core node when you are only using a single core.</p>
</div>
</div>
<div class="section" id="running-mpi-parallel-jobs">
<h2>Running MPI parallel jobs<a class="headerlink" href="#running-mpi-parallel-jobs" title="Permalink to this headline">¶</a></h2>
<p>When you running parallel jobs requiring MPI you will use an MPI launch
command to start your executable in parallel. The name and options for
this MPI launch command depend on which MPI library you are using:
SGI MPT (Message Passing Toolkit) or Intel MPI. We give details below
of the commands used in each case and our example job submission scripts
have examples for both libraries.</p>
<p><strong>Note:</strong> If you are using a centrally-installed MPI software package you
will need to know which MPI library was used to compile it so you can use the
correct MPI launch command. You can find this information using the <code class="docutils literal"><span class="pre">module</span> <span class="pre">show</span></code>
command. For example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>[auser@cirrus-login0 ~]$ module show vasp
-------------------------------------------------------------------
/lustre/sw/modulefiles/vasp/5.4.4-intel17-mpt214:

conflict      vasp
module                load mpt
module                load intel-compilers-17
module                load intel-cmkl-17
module                load gcc/6.2.0
prepend-path  PATH /lustre/home/y07/vasp5/5.4.4-intel17-mpt214/bin
setenv                VASP5 /lustre/home/y07/vasp5/5.4.4-intel17-mpt214
setenv                VASP5_VDW_KERNEL /lustre/home/y07/vasp5/5.4.4-intel17-mpt214/vdw_kernal/vdw_kernel.bindat
-------------------------------------------------------------------
</pre></div>
</div>
<p>This shows that VASP was compiled with SGI MPT (from the <code class="docutils literal"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">mpt</span></code> in
the output from the command. If a package was compiled with Intel MPI there
would be <code class="docutils literal"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">intel-mpi-17</span></code> in the output instead.</p>
<div class="section" id="sgi-mpt-message-passing-toolkit">
<h3>SGI MPT (Message Passing Toolkit)<a class="headerlink" href="#sgi-mpt-message-passing-toolkit" title="Permalink to this headline">¶</a></h3>
<p>SGI MPT is accessed at both compile and runtime by loading the <code class="docutils literal"><span class="pre">mpt</span></code> module:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="n">load</span> <span class="n">mpt</span>
</pre></div>
</div>
<div class="section" id="sgi-mpt-parallel-launcher-mpiexec-mpt">
<h4>SGI MPT: parallel launcher <code class="docutils literal"><span class="pre">mpiexec_mpt</span></code><a class="headerlink" href="#sgi-mpt-parallel-launcher-mpiexec-mpt" title="Permalink to this headline">¶</a></h4>
<p>The SGI MPT parallel launcher on Cirrus is <code class="docutils literal"><span class="pre">mpiexec_mpt</span></code>.</p>
<p><strong>Note:</strong> this parallel job launcher is only available once you have
loaded the <code class="docutils literal"><span class="pre">mpt</span></code> module.</p>
<p>A sample MPI launch line using <code class="docutils literal"><span class="pre">mpiexec_mpt</span></code> looks like:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">mpiexec_mpt</span> <span class="o">-</span><span class="n">n</span> <span class="mi">72</span> <span class="o">-</span><span class="n">ppn</span> <span class="mi">36</span> <span class="o">./</span><span class="n">my_mpi_executable</span><span class="o">.</span><span class="n">x</span> <span class="n">arg1</span> <span class="n">arg2</span>
</pre></div>
</div>
<p>This will start the parallel executable “my_mpi_executable.x” with
arguments “arg1” and “arg2”. The job will be started using 72 MPI
processes, with 36 MPI processes are placed on each compute node
(this would use all the physical cores on each node). This would
require 2 nodes to be requested in the PBS options.</p>
<p>The most important <code class="docutils literal"><span class="pre">mpiexec_mpt</span></code> flags are:</p>
<blockquote>
<div><dl class="docutils">
<dt><code class="docutils literal"><span class="pre">-n</span> <span class="pre">[total</span> <span class="pre">number</span> <span class="pre">of</span> <span class="pre">MPI</span> <span class="pre">processes]</span></code></dt>
<dd>Specifies the total number of distributed memory parallel processes
(not including shared-memory threads). For jobs that use all
physical cores this will usually be a multiple of 36. The default on
Cirrus is 1.</dd>
<dt><code class="docutils literal"><span class="pre">-ppn</span> <span class="pre">[parallel</span> <span class="pre">processes</span> <span class="pre">per</span> <span class="pre">node]</span></code></dt>
<dd>Specifies the number of distributed memory parallel processes per
node. There is a choice of 1-36 for physical cores on Cirrus compute
nodes (1-72 if you are using Hyper-Threading) If you are running with
exclusive node usage, the most economic choice is always to run with
“fully-packed” nodes on all physical cores if possible, i.e.
<code class="docutils literal"><span class="pre">-ppn</span> <span class="pre">36</span></code> . Running “unpacked” or “underpopulated” (i.e. not using
all the physical cores on a node) is useful if you need large
amounts of memory per parallel process or you are using more than
one shared-memory thread per parallel process.</dd>
</dl>
</div></blockquote>
<p><strong>Note:</strong> <code class="docutils literal"><span class="pre">mpiexec_mpt</span></code> only works from within a PBS job submission script.</p>
<p>Please use <code class="docutils literal"><span class="pre">man</span> <span class="pre">mpiexec_mpt</span></code> query further options. (This is only available
once you have loaded the <code class="docutils literal"><span class="pre">mpt</span></code> module.)</p>
</div>
<div class="section" id="sgi-mpt-interactive-mpi-using-mpirun">
<h4>SGI MPT: interactive MPI using <code class="docutils literal"><span class="pre">mpirun</span></code><a class="headerlink" href="#sgi-mpt-interactive-mpi-using-mpirun" title="Permalink to this headline">¶</a></h4>
<p>If you want to run short interactive parallel applications (e.g. for
debugging) then you can run SGI MPT compiled MPI applications on the login
nodes using the <code class="docutils literal"><span class="pre">mpirun</span></code> command.</p>
<p>For instance, to run a simple, short 4-way MPI job on the login node, issue the
following command (once you have loaded the appropriate modules):</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">mpirun</span> <span class="o">-</span><span class="n">n</span> <span class="mi">4</span> <span class="o">./</span><span class="n">hello_mpi</span><span class="o">.</span><span class="n">x</span>
</pre></div>
</div>
<p><strong>Note:</strong> you should not run long, compute- or memory-intensive jobs on the
login nodes. Any such processes are liable to termination by the system
with no warning.</p>
</div>
<div class="section" id="sgi-mpt-running-hybrid-mpi-openmp-applications">
<h4>SGI MPT: running hybrid MPI/OpenMP applications<a class="headerlink" href="#sgi-mpt-running-hybrid-mpi-openmp-applications" title="Permalink to this headline">¶</a></h4>
<p>If you are running hybrid MPI/OpenMP code using SGI MPT you will also often make
use of the <code class="docutils literal"><span class="pre">omplace</span></code> tool in your job launcher line. This tool
takes the number of threads as the option <code class="docutils literal"><span class="pre">-nt</span></code>:</p>
<blockquote>
<div><dl class="docutils">
<dt><code class="docutils literal"><span class="pre">-nt</span> <span class="pre">[threads</span> <span class="pre">per</span> <span class="pre">parallel</span> <span class="pre">process]</span></code></dt>
<dd>Specifies the number of cores for each parallel process to use for
shared-memory threading. (This is in addition to the
<code class="docutils literal"><span class="pre">OMP_NUM_THREADS</span></code> environment variable if you are using OpenMP for
your shared memory programming.) The default on Cirrus is 1.</dd>
</dl>
</div></blockquote>
<p>Please use <code class="docutils literal"><span class="pre">man</span> <span class="pre">mpiexec_mpt</span></code> and <code class="docutils literal"><span class="pre">man</span> <span class="pre">omplace</span></code> to query further options.
(Again, these are only available once you have loaded the <code class="docutils literal"><span class="pre">mpt</span></code> module.)</p>
</div>
</div>
<div class="section" id="intel-mpi">
<h3>Intel MPI<a class="headerlink" href="#intel-mpi" title="Permalink to this headline">¶</a></h3>
<p>Intel MPI is accessed at runtime by loading the <code class="docutils literal"><span class="pre">intel-mpi-17</span></code>.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="n">load</span> <span class="n">intel</span><span class="o">-</span><span class="n">mpi</span><span class="o">-</span><span class="mi">17</span>
</pre></div>
</div>
<div class="section" id="intel-mpi-parallel-job-launcher-mpirun">
<h4>Intel MPI: parallel job launcher <code class="docutils literal"><span class="pre">mpirun</span></code><a class="headerlink" href="#intel-mpi-parallel-job-launcher-mpirun" title="Permalink to this headline">¶</a></h4>
<p>The Intel MPI parallel job launcher on Cirrus is <code class="docutils literal"><span class="pre">mpirun</span></code>.</p>
<p><strong>Note:</strong> this parallel job launcher is only available once you have
loaded the <code class="docutils literal"><span class="pre">intel-mpi-17</span></code> module.</p>
<p>A sample MPI launch line using <code class="docutils literal"><span class="pre">mpirun</span></code> looks like:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">mpirun</span> <span class="o">-</span><span class="n">n</span> <span class="mi">72</span> <span class="o">-</span><span class="n">ppn</span> <span class="mi">36</span> <span class="o">./</span><span class="n">my_mpi_executable</span><span class="o">.</span><span class="n">x</span> <span class="n">arg1</span> <span class="n">arg2</span>
</pre></div>
</div>
<p>This will start the parallel executable “my_mpi_executable.x” with
arguments “arg1” and “arg2”. The job will be started using 72 MPI
processes, with 36 MPI processes are placed on each compute node
(this would use all the physical cores on each node). This would
require 2 nodes to be requested in the PBS options.</p>
<p>The most important <code class="docutils literal"><span class="pre">mpirun</span></code> flags are:</p>
<blockquote>
<div><dl class="docutils">
<dt><code class="docutils literal"><span class="pre">-n</span> <span class="pre">[total</span> <span class="pre">number</span> <span class="pre">of</span> <span class="pre">MPI</span> <span class="pre">processes]</span></code></dt>
<dd>Specifies the total number of distributed memory parallel processes
(not including shared-memory threads). For jobs that use all
physical cores this will usually be a multiple of 36. The default on
Cirrus is 1.</dd>
<dt><code class="docutils literal"><span class="pre">-ppn</span> <span class="pre">[parallel</span> <span class="pre">processes</span> <span class="pre">per</span> <span class="pre">node]</span></code></dt>
<dd>Specifies the number of distributed memory parallel processes per
node. There is a choice of 1-36 for physical cores on Cirrus compute
nodes (1-72 if you are using Hyper-Threading) If you are running with
exclusive node usage, the most economic choice is always to run with
“fully-packed” nodes on all physical cores if possible, i.e.
<code class="docutils literal"><span class="pre">-ppn</span> <span class="pre">36</span></code> . Running “unpacked” or “underpopulated” (i.e. not using
all the physical cores on a node) is useful if you need large
amounts of memory per parallel process or you are using more than
one shared-memory thread per parallel process.</dd>
</dl>
</div></blockquote>
<p>Documentation on using Intel MPI (including <code class="docutils literal"><span class="pre">mpirun</span></code>) can be found
online at:</p>
<ul class="simple">
<li><a class="reference external" href="https://software.intel.com/en-us/articles/intel-mpi-library-documentation">Intel MPI Documentation</a></li>
</ul>
</div>
<div class="section" id="intel-mpi-running-hybrid-mpi-openmp-applications">
<h4>Intel MPI: running hybrid MPI/OpenMP applications<a class="headerlink" href="#intel-mpi-running-hybrid-mpi-openmp-applications" title="Permalink to this headline">¶</a></h4>
<p>If you are running hybrid MPI/OpenMP code using Intel MPI you need to
set the <code class="docutils literal"><span class="pre">I_MPI_PIN_DOMAIN</span></code> environment variable to <code class="docutils literal"><span class="pre">omp</span></code> so that
MPI tasks are pinned with enough space for OpenMP threads.</p>
<p>For example, in your job submission script you would use:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">I_MPI_PIN_DOMAIN</span><span class="o">=</span><span class="n">omp</span>
</pre></div>
</div>
<p>You can then also use the <code class="docutils literal"><span class="pre">KMP_AFFINITY</span></code> enviroment variable
to control placement of OpenMP threads. For more information, see:</p>
<ul class="simple">
<li><a class="reference external" href="https://software.intel.com/en-us/articles/openmp-thread-affinity-control">Intel OpenMP Thread Affinity Control</a></li>
</ul>
</div>
<div class="section" id="intel-mpi-mpi-io-setup">
<h4>Intel MPI: MPI-IO setup<a class="headerlink" href="#intel-mpi-mpi-io-setup" title="Permalink to this headline">¶</a></h4>
<p>If you wish to use MPI-IO with Intel MPI you must set a couple of
additional environment variables in your job submission script to
tell the MPI library to use the Lustre file system interface.
Specifically, you should add the lines:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">I_MPI_EXTRA_FILESYSTEM</span><span class="o">=</span><span class="n">on</span>
<span class="n">export</span> <span class="n">I_MPI_EXTRA_FILESYSTEM_LIST</span><span class="o">=</span><span class="n">lustre</span>
</pre></div>
</div>
<p>after you have loaded the <code class="docutils literal"><span class="pre">intel-mpi-17</span></code> module.</p>
<p>If you fail to set these environment variables you may see errors such as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">This</span> <span class="n">requires</span> <span class="n">fcntl</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="n">to</span> <span class="n">be</span> <span class="n">implemented</span><span class="o">.</span> <span class="n">As</span> <span class="n">of</span> <span class="mi">8</span><span class="o">/</span><span class="mi">25</span><span class="o">/</span><span class="mi">2011</span> <span class="n">it</span> <span class="ow">is</span> <span class="ow">not</span><span class="o">.</span> <span class="n">Generic</span> <span class="n">MPICH</span>
<span class="n">Message</span><span class="p">:</span> <span class="n">File</span> <span class="n">locking</span> <span class="n">failed</span> <span class="ow">in</span>
<span class="n">ADIOI_Set_lock</span><span class="p">(</span><span class="n">fd</span> <span class="mi">0</span><span class="p">,</span><span class="n">cmd</span> <span class="n">F_SETLKW</span><span class="o">/</span><span class="mi">7</span><span class="p">,</span><span class="nb">type</span> <span class="n">F_WRLCK</span><span class="o">/</span><span class="mi">1</span><span class="p">,</span><span class="n">whence</span> <span class="mi">0</span><span class="p">)</span> <span class="k">with</span> <span class="k">return</span> <span class="n">value</span>
<span class="n">FFFFFFFF</span> <span class="ow">and</span> <span class="n">errno</span> <span class="mf">26.</span>
<span class="o">-</span> <span class="n">If</span> <span class="n">the</span> <span class="n">file</span> <span class="n">system</span> <span class="ow">is</span> <span class="n">NFS</span><span class="p">,</span> <span class="n">you</span> <span class="n">need</span> <span class="n">to</span> <span class="n">use</span> <span class="n">NFS</span> <span class="n">version</span> <span class="mi">3</span><span class="p">,</span> <span class="n">ensure</span> <span class="n">that</span> <span class="n">the</span> <span class="n">lockd</span>
 <span class="n">daemon</span> <span class="ow">is</span> <span class="n">running</span> <span class="n">on</span> <span class="nb">all</span> <span class="n">the</span> <span class="n">machines</span><span class="p">,</span> <span class="ow">and</span> <span class="n">mount</span> <span class="n">the</span> <span class="n">directory</span> <span class="k">with</span> <span class="n">the</span> <span class="s1">&#39;noac&#39;</span>
 <span class="n">option</span> <span class="p">(</span><span class="n">no</span> <span class="n">attribute</span> <span class="n">caching</span><span class="p">)</span><span class="o">.</span>
<span class="o">-</span> <span class="n">If</span> <span class="n">the</span> <span class="n">file</span> <span class="n">system</span> <span class="ow">is</span> <span class="n">LUSTRE</span><span class="p">,</span> <span class="n">ensure</span> <span class="n">that</span> <span class="n">the</span> <span class="n">directory</span> <span class="ow">is</span> <span class="n">mounted</span> <span class="k">with</span> <span class="n">the</span> <span class="s1">&#39;flock&#39;</span>
 <span class="n">option</span><span class="o">.</span>
<span class="n">ADIOI_Set_lock</span><span class="p">::</span> <span class="n">Function</span> <span class="ow">not</span> <span class="n">implemented</span>
<span class="n">ADIOI_Set_lock</span><span class="p">:</span><span class="n">offset</span> <span class="mi">0</span><span class="p">,</span> <span class="n">length</span> <span class="mi">10</span>
<span class="n">application</span> <span class="n">called</span> <span class="n">MPI_Abort</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">process</span> <span class="mi">3</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="example-parallel-mpi-job-submission-scripts">
<h2>Example parallel MPI job submission scripts<a class="headerlink" href="#example-parallel-mpi-job-submission-scripts" title="Permalink to this headline">¶</a></h2>
<p>A subset of example job submssion scripts are included in full below. The
full set are available via the following links:</p>
<ul class="simple">
<li>SGI MPT MPI Job: <a class="reference download internal" href="../_downloads/example_mpi_sgimpt.bash" download=""><code class="xref download docutils literal"><span class="pre">example_mpi_sgimpt.bash</span></code></a></li>
<li>Intel MPI Job: <a class="reference download internal" href="../_downloads/example_mpi_impi.bash" download=""><code class="xref download docutils literal"><span class="pre">example_mpi_impi.bash</span></code></a></li>
<li>SGI MPT Hybrid MPI/OpenMP Job: <a class="reference download internal" href="../_downloads/example_hybrid_sgimpt.bash" download=""><code class="xref download docutils literal"><span class="pre">example_hybrid_sgimpt.bash</span></code></a></li>
<li>Intel MPI Hybrid MPI/OpenMP Job: <a class="reference download internal" href="../_downloads/example_hybrid_impi.bash" download=""><code class="xref download docutils literal"><span class="pre">example_hybrid_impi.bash</span></code></a></li>
</ul>
<div class="section" id="example-sgi-mpt-job-submission-script-for-mpi-parallel-job">
<h3>Example: SGI MPT job submission script for MPI parallel job<a class="headerlink" href="#example-sgi-mpt-job-submission-script-for-mpi-parallel-job" title="Permalink to this headline">¶</a></h3>
<p>A simple MPI job submission script to submit a job using 4 compute
nodes (maximum of 144 physical cores) for 20 minutes would look like:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>#!/bin/bash --login

# PBS job options (name, compute nodes, job time)
#PBS -N Example_MPI_Job
# Select 4 full nodes
#PBS -l select=4:ncpus=72
# Parallel jobs should always specify exclusive node access
#PBS -l place=excl
#PBS -l walltime=00:20:00

# Replace [budget code] below with your project code (e.g. t01)
#PBS -A [budget code]

# Change to the directory that the job was submitted from
cd $PBS_O_WORKDIR

# Load any required modules
module load mpt
module load intel-compilers-17

# Set the number of threads to 1
#   This prevents any threaded system libraries from automatically
#   using threading.
export OMP_NUM_THREADS=1

# Launch the parallel job
#   Using 144 MPI processes and 36 MPI processes per node
mpiexec_mpt -n 144 -ppn 36 ./my_mpi_executable.x arg1 arg2 &gt; my_stdout.txt 2&gt; my_stderr.txt
</pre></div>
</div>
<p>This will run your executable “my_mpi_executable.x” in parallel on 144
MPI processes using 2 nodes (36 cores per node, i.e. not using hyper-threading). PBS will
allocate 4 nodes to your job and mpirun_mpt will place 36 MPI processes on each node
(one per physical core).</p>
<p>See above for a more detailed discussion of the different PBS options</p>
</div>
<div class="section" id="example-sgi-mpt-job-submission-script-for-mpi-openmp-mixed-mode-parallel-job">
<h3>Example: SGI MPT job submission script for MPI+OpenMP (mixed mode) parallel job<a class="headerlink" href="#example-sgi-mpt-job-submission-script-for-mpi-openmp-mixed-mode-parallel-job" title="Permalink to this headline">¶</a></h3>
<p>Mixed mode codes that use both MPI (or another distributed memory
parallel model) and OpenMP should take care to ensure that the shared
memory portion of the process/thread placement does not span more than
one node. This means that the number of shared memory threads should be
a factor of 18.</p>
<p>In the example below, we are using 4 nodes for 6 hours. There are 4 MPI
processes in total and 18 OpenMP threads per MPI process. Note the use
of the <code class="docutils literal"><span class="pre">omplace</span></code> command to specify the number of threads.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>#!/bin/bash --login

# PBS job options (name, compute nodes, job time)
#PBS -N Example_MixedMode_Job
# Select 4 full nodes
#PBS -l select=4:ncpus=72
# Parallel jobs should always specify exclusive node access
#PBS -l place=excl
#PBS -l walltime=6:0:0

# Replace [budget code] below with your project code (e.g. t01)
#PBS -A [budget code]

# Change to the directory that the job was submitted from
cd $PBS_O_WORKDIR

# Load any required modules
module load mpt
module load intel-compilers-17

# Set the number of threads to 18
#   There are 18 OpenMP threads per MPI process
export OMP_NUM_THREADS=18

# Launch the parallel job
#   Using 8 MPI processes
#   2 MPI processes per node
#   18 OpenMP threads per MPI process
mpiexec_mpt -n 8 -ppn 2 omplace -nt 18 ./my_mixed_executable.x arg1 arg2 &gt; my_stdout.txt 2&gt; my_stderr.txt
</pre></div>
</div>
</div>
<div class="section" id="example-job-submission-script-for-parallel-non-mpi-based-jobs">
<h3>Example: job submission script for parallel non-MPI based jobs<a class="headerlink" href="#example-job-submission-script-for-parallel-non-mpi-based-jobs" title="Permalink to this headline">¶</a></h3>
<p>If you want to run on multiple nodes, where each node is running a self-contained job, not using MPI
(e.g.) for processing data or a parameter sweep, you can use the SGI MPT <code class="docutils literal"><span class="pre">mpiexec_mpt</span></code> launcher to control job placement.</p>
<p>In the example script below, <code class="docutils literal"><span class="pre">work.bash</span></code> is a bash script which runs a threaded executable with a command-line input and
<code class="docutils literal"><span class="pre">perf.bash</span></code> is a bash script which copies data from the CPU performance counters to an output file. As both handle the
threading themselves, it is sufficient to allocate 1 MPI rank. Using the ampersand <code class="docutils literal"><span class="pre">&amp;</span></code> allows both to execute simultaneously.
Both <code class="docutils literal"><span class="pre">work.bash</span></code> and <code class="docutils literal"><span class="pre">perf.bash</span></code> run on 4 nodes.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>#!/bin/bash --login
# PBS job options (name, compute nodes, job time)
#PBS -N Example_MixedMode_Job
# Select 4 full nodes
#PBS -l select=4:ncpus=72
# Parallel jobs should always specify exclusive node access
#PBS -l place=excl
#PBS -l walltime=6:0:0

# Replace [budget code] below with your project code (e.g. t01)
#PBS -A [budget code]

# Change to the directory that the job was submitted from
cd $PBS_O_WORKDIR

# Load any required modules
module load mpt

# Set this variable to inform mpiexec_mpt these are not MPI jobs
export MPI_SHEPHERD=true

# Execute work and perf scripts on nodes simultaneously.
mpiexec_mpt -n 4 -ppn 1 work.bash &amp;
mpiexec_mpt -n 4 -ppn 1 perf.bash &amp;
wait
</pre></div>
</div>
<p><strong>Note:</strong> the <code class="docutils literal"><span class="pre">wait</span></code> command is required to stop the PBS job finishing before the scripts finish.
If you find odd behaviour, especially with respect to the values of bash variables, double check you
have set <code class="docutils literal"><span class="pre">MPI_SHEPHERD=true</span></code></p>
</div>
</div>
<div class="section" id="serial-jobs">
<h2>Serial Jobs<a class="headerlink" href="#serial-jobs" title="Permalink to this headline">¶</a></h2>
<p>Serial jobs are setup in a similar way to parallel jobs on Cirrus. The
only changes are:</p>
<ol class="arabic simple">
<li>You should request a single core with <code class="docutils literal"><span class="pre">select=1:ncpus=1</span></code></li>
<li>You will not need to use a parallel job launcher to run your executable</li>
<li>You will generally not specify exclusive node access</li>
</ol>
<p>A simple serial script to compress a file would be:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>#!/bin/bash --login

# PBS job options (name, compute nodes, job time)
#PBS -N Example_Serial_Job
#PBS -l select=1:ncpus=1
#PBS -l walltime=0:20:0

# Replace [budget code] below with your project code (e.g. t01)
#PBS -A [budget code]

# Change to the directory that the job was submitted from
cd $PBS_O_WORKDIR

# Load any required modules
module load intel-compilers-16

# Set the number of threads to 1 to ensure serial
export OMP_NUM_THREADS=1

# Run the serial executable
gzip my_big_file.dat
</pre></div>
</div>
</div>
<div class="section" id="job-arrays">
<span id="jobarrays"></span><h2>Job arrays<a class="headerlink" href="#job-arrays" title="Permalink to this headline">¶</a></h2>
<p>The PBSPro job scheduling system offers the <em>job array</em> concept,
for running collections of almost-identical jobs, for example
running the same program several times with different arguments
or input data.</p>
<p>Each job in a job array is called a <em>subjob</em>.  The subjobs of a job
array can be submitted and queried as a unit, making it easier and
cleaner to handle the full set, compared to individual jobs.</p>
<p>All subjobs in a job array are started by running the same job script.
The job script also contains information on the number of jobs to be
started, and PBSPro provides a subjob index which can be passed to
the individual subjobs or used to select the input data per subjob.</p>
<div class="section" id="job-script-for-a-job-array">
<h3>Job script for a job array<a class="headerlink" href="#job-script-for-a-job-array" title="Permalink to this headline">¶</a></h3>
<p>As an example, to start 56 subjobs, with the subjob index as the only
argument, and 4 hours maximum runtime per subjob, save the following
content into the file job_script.pbs:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>#!/bin/bash --login
#PBS -l select=1:ncpus=1
#PBS -l walltime=04:00:00
#PBS -J 1-56
#PBS -q workq
#PBS -V

cd ${PBS_O_WORKDIR}

/path/to/exe $PBS_ARRAY_INDEX
</pre></div>
</div>
<p>Another example of a job script for submitting a job array is given
<a class="reference external" href="../software-packages/flacs.html#submitting-many-flacs-jobs-as-a-job-array">here</a>.</p>
</div>
<div class="section" id="starting-a-job-array">
<h3>Starting a job array<a class="headerlink" href="#starting-a-job-array" title="Permalink to this headline">¶</a></h3>
<p>When starting a job array, most options can be included in the job
file, but the project code for the resource billing has to be
specified on the command line:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">qsub</span> <span class="o">-</span><span class="n">A</span> <span class="p">[</span><span class="n">project</span> <span class="n">code</span><span class="p">]</span> <span class="n">job_script</span><span class="o">.</span><span class="n">pbs</span>
</pre></div>
</div>
</div>
<div class="section" id="querying-a-job-array">
<h3>Querying a job array<a class="headerlink" href="#querying-a-job-array" title="Permalink to this headline">¶</a></h3>
<p>In the normal PBSPro job status, a job array will be shown as a single
line:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">qstat</span>
<span class="n">Job</span> <span class="nb">id</span>            <span class="n">Name</span>           <span class="n">User</span>   <span class="n">Time</span> <span class="n">Use</span> <span class="n">S</span> <span class="n">Queue</span>
<span class="o">----------------</span>  <span class="o">--------------</span> <span class="o">------</span> <span class="o">--------</span> <span class="o">-</span> <span class="o">-----</span>
<span class="mi">112452</span><span class="p">[]</span><span class="o">.</span><span class="n">indy2</span><span class="o">-</span><span class="n">lo</span> <span class="n">dispsim</span>        <span class="n">user1</span>         <span class="mi">0</span> <span class="n">B</span> <span class="n">workq</span>
</pre></div>
</div>
<p>To monitor the subjobs of the job 112452, use</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">qstat</span> <span class="o">-</span><span class="n">t</span> <span class="mi">1235</span><span class="p">[]</span>
<span class="n">Job</span> <span class="nb">id</span>            <span class="n">Name</span>             <span class="n">User</span>              <span class="n">Time</span> <span class="n">Use</span> <span class="n">S</span> <span class="n">Queue</span>
<span class="o">----------------</span>  <span class="o">----------------</span> <span class="o">----------------</span>  <span class="o">--------</span> <span class="o">-</span> <span class="o">-----</span>
<span class="mi">112452</span><span class="p">[]</span><span class="o">.</span><span class="n">indy2</span><span class="o">-</span><span class="n">lo</span> <span class="n">dispsim</span>          <span class="n">user1</span>                    <span class="mi">0</span> <span class="n">B</span> <span class="n">flacs</span>
<span class="mi">112452</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">indy2</span><span class="o">-</span><span class="n">l</span> <span class="n">dispsim</span>          <span class="n">user1</span>             <span class="mi">02</span><span class="p">:</span><span class="mi">45</span><span class="p">:</span><span class="mi">37</span> <span class="n">R</span> <span class="n">flacs</span>
<span class="mi">112452</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">indy2</span><span class="o">-</span><span class="n">l</span> <span class="n">dispsim</span>          <span class="n">user1</span>             <span class="mi">02</span><span class="p">:</span><span class="mi">45</span><span class="p">:</span><span class="mi">56</span> <span class="n">R</span> <span class="n">flacs</span>
<span class="mi">112452</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">indy2</span><span class="o">-</span><span class="n">l</span> <span class="n">dispsim</span>          <span class="n">user1</span>             <span class="mi">02</span><span class="p">:</span><span class="mi">45</span><span class="p">:</span><span class="mi">33</span> <span class="n">R</span> <span class="n">flacs</span>
<span class="mi">112452</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">indy2</span><span class="o">-</span><span class="n">l</span> <span class="n">dispsim</span>          <span class="n">user1</span>             <span class="mi">02</span><span class="p">:</span><span class="mi">45</span><span class="p">:</span><span class="mi">45</span> <span class="n">R</span> <span class="n">flacs</span>
<span class="mi">112452</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">indy2</span><span class="o">-</span><span class="n">l</span> <span class="n">dispsim</span>          <span class="n">user1</span>             <span class="mi">02</span><span class="p">:</span><span class="mi">45</span><span class="p">:</span><span class="mi">26</span> <span class="n">R</span> <span class="n">flacs</span>
<span class="o">...</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="interactive-jobs">
<h2>Interactive Jobs<a class="headerlink" href="#interactive-jobs" title="Permalink to this headline">¶</a></h2>
<p>When you are developing or debugging code you often want to run many
short jobs with a small amount of editing the code between runs. This
can be achieved by using the login nodes to run MPI but you may want
to test on the compute nodes (e.g. you may want to test running on
multiple nodes across the high performance interconnect). One of the
best ways to achieve this on Cirrus is to use interactive jobs.</p>
<p>An interactive job allows you to issue <code class="docutils literal"><span class="pre">mpirun_mpt</span></code> commands directly
from the command line without using a job submission script, and to
see the output from your program directly in the terminal.</p>
<p>To submit a request for an interactive job reserving 8 nodes
(288 physical cores) for 1 hour you would
issue the following qsub command from the command line:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">qsub</span> <span class="o">-</span><span class="n">IVl</span> <span class="n">select</span><span class="o">=</span><span class="mi">8</span><span class="p">:</span><span class="n">ncpus</span><span class="o">=</span><span class="mi">72</span><span class="p">,</span><span class="n">walltime</span><span class="o">=</span><span class="mi">1</span><span class="p">:</span><span class="mi">0</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span><span class="n">place</span><span class="o">=</span><span class="n">excl</span> <span class="o">-</span><span class="n">A</span> <span class="p">[</span><span class="n">project</span> <span class="n">code</span><span class="p">]</span>
</pre></div>
</div>
<p>When you submit this job your terminal will display something like:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">qsub</span><span class="p">:</span> <span class="n">waiting</span> <span class="k">for</span> <span class="n">job</span> <span class="mf">19366.</span><span class="n">indy2</span><span class="o">-</span><span class="n">login0</span> <span class="n">to</span> <span class="n">start</span>
</pre></div>
</div>
<p>It may take some time for your interactive job to start. Once it
runs you will enter a standard interactive terminal session.
Whilst the interactive session lasts you will be able to run parallel
jobs on the compute nodes by issuing the <code class="docutils literal"><span class="pre">mpirun_mpt</span></code>  command
directly at your command prompt (remember you will need to load the
<code class="docutils literal"><span class="pre">mpt</span></code> module and any compiler modules before running)  using the
same syntax as you would inside a job script. The maximum number
of cores you can use is limited by the value of select you specify
when you submit a request for the interactive job.</p>
<p>If you know you will be doing a lot of intensive debugging you may
find it useful to request an interactive session lasting the expected
length of your working session, say a full day.</p>
<p>Your session will end when you hit the requested walltime. If you
wish to finish before this you should use the <code class="docutils literal"><span class="pre">exit</span></code> command.</p>
</div>
<div class="section" id="reservations">
<h2>Reservations<a class="headerlink" href="#reservations" title="Permalink to this headline">¶</a></h2>
<p>Resource reservations are available on Cirrus. These allow users to reserve
a number of nodes for a specified length of time starting at a particular
time on the system.</p>
<p>Examples of the reasons for using reservations could be:</p>
<ul class="simple">
<li>An exceptional job requires longer than 96 hours runtime.</li>
<li>You require a job/jobs to run at a particular time e.g. for a demonstration or course.</li>
</ul>
<p><strong>Note:</strong> Reservations will be charged at 1.5 times the usual rate and you
will be charged the full rate for the entire reservation whether or not you use the
resources reserved for the full time. In addition, you will not be refunded the resources
if you fail to use them due to a job crash unless this crash is due to a system failure.</p>
<div class="section" id="requesting-reservations">
<h3>Requesting reservations<a class="headerlink" href="#requesting-reservations" title="Permalink to this headline">¶</a></h3>
<p>You request a reservation on Cirrus using PBS from the command line. Before
requesting the reservation, you will need the following information:</p>
<ul class="simple">
<li>The start time for the resevation</li>
<li>The duration of the reservation</li>
<li>The number of cores (or nodes for multi-node, node-exclusive jobs)</li>
<li>The project ID you wish to charge the reservation to</li>
</ul>
<p>You use the <code class="docutils literal"><span class="pre">pbs_rsub</span></code> command to create a reservation. This command has a similar
syntax to the <code class="docutils literal"><span class="pre">qsub</span></code> command for requesting resources but takes the additional
parameters <code class="docutils literal"><span class="pre">-R</span></code> (to specify the reservaiton start time); <code class="docutils literal"><span class="pre">-D</span></code> (to specify the reservation
duration); and <code class="docutils literal"><span class="pre">-G</span></code> (to specify the project ID to charge the reservation to). For example,
to create a reservation for 3 hours at 10:30 (UK time) on Saturday 26 August 2017 for 4
full nodes (144 physical cores, 288 hyperthreads) and charge to project “t01” you would use the command:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>[auser@cirrus-login0 ~]$ pbs_rsub -R 1708261030 -D 3:0:0 -l select=4:ncpus=72,place=excl -G +t01
R122604.indy2-login0 UNCONFIRMED
</pre></div>
</div>
<p>The command will return a reservation ID (<code class="docutils literal"><span class="pre">R122604</span></code> in the example above) and note that
it is currently <code class="docutils literal"><span class="pre">UNCONFIRMED</span></code>. PBSPro will change the status to <code class="docutils literal"><span class="pre">CONFIRMED</span></code> once it
has checked that it is possible to schedule the reservation.</p>
<p><strong>Note:</strong> Only the user that requested this reservation will be able to submit jobs to it. To
create a reservation that is available to all users in a particular project, see the instructions
below.</p>
<p>There are many other options to the <code class="docutils literal"><span class="pre">pbs_rsub</span></code> command. Please check the man page for
a full description.</p>
</div>
<div class="section" id="checking-the-status-of-your-reservation">
<h3>Checking the status of your reservation<a class="headerlink" href="#checking-the-status-of-your-reservation" title="Permalink to this headline">¶</a></h3>
<p>You can cheack the status of your reservation request with the <code class="docutils literal"><span class="pre">pbs_rstat</span></code> command:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>[auser@cirrus-login0 ~]$ pbs_rstat
Resv ID    Queue    User     State             Start / Duration / End
---------------------------------------------------------------------
R122604.in R122605  aturner@ CO            Sat 10:30 / 10800 / Sat 13:30
</pre></div>
</div>
<p>and, as you can see, the status of the requested reservation is now <code class="docutils literal"><span class="pre">CO</span></code> (<code class="docutils literal"><span class="pre">CONFIRMED</span></code>).</p>
</div>
<div class="section" id="submitting-jobs-to-a-reservation">
<h3>Submitting jobs to a reservation<a class="headerlink" href="#submitting-jobs-to-a-reservation" title="Permalink to this headline">¶</a></h3>
<p>You submit jobs to reservations in the same way as you do for all other jobs using the
<code class="docutils literal"><span class="pre">qsub</span></code> command. The only additional information required is to specify the reservation
ID to the <code class="docutils literal"><span class="pre">-q</span></code> option. For example, to submit to the reservation created above you would
use:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">qsub</span> <span class="o">-</span><span class="n">q</span> <span class="n">R122604</span> <span class="o">...</span><span class="n">usual</span> <span class="n">qsub</span> <span class="n">options</span><span class="o">/</span><span class="n">job</span> <span class="n">script</span> <span class="n">name</span><span class="o">...</span>
</pre></div>
</div>
<p><strong>Note:</strong> You can submit jobs to the reservation ahead of the start time and the job will
start as soon as the reservation begins.</p>
</div>
<div class="section" id="reservations-for-all-project-users">
<h3>Reservations for all project users<a class="headerlink" href="#reservations-for-all-project-users" title="Permalink to this headline">¶</a></h3>
<p>By default, a reservation will only be available to the user who requested it. If you wish
to create a reservation that is usable by all members of your project you need to modify
the user permissions using the <code class="docutils literal"><span class="pre">-U</span></code> option.</p>
<p>For example, to create a reservation for 192 hours, starting at 16:15 (UK time) on Monday 18
September 2017 for 64 nodes accessible by all users in the t01 project you would use:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>[auser@cirrus-login0 ~]$ pbs_rsub -R 1709181615 -D 192:0:0 -l select=64:ncpus=72,place=excl -G +t01 -U +
R122605.indy2-login0 UNCONFIRMED
</pre></div>
</div>
<p>Here, the <code class="docutils literal"><span class="pre">-G</span> <span class="pre">+t01</span></code> option charges the reservation to the t01 project <strong>and</strong> restricts access to
users in the <code class="docutils literal"><span class="pre">t01</span></code> project; the <code class="docutils literal"><span class="pre">-U</span> <span class="pre">+</span></code> option allows all users (in the t01 project) access
to the reservation.</p>
<p><strong>Note:</strong> You can restrict access to specific users within a project, see the pbs_rsub man
page for more information on how to do this.</p>
</div>
<div class="section" id="deleting-a-reservation">
<h3>Deleting a reservation<a class="headerlink" href="#deleting-a-reservation" title="Permalink to this headline">¶</a></h3>
<p>Use the <code class="docutils literal"><span class="pre">pbs_rdel</span></code> command to delete a reservation:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>[auser@cirrus-login0 ~]$ pbs_rdel R122605
</pre></div>
</div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/cirrus_logo_white-Transparent-Background.png" alt="Logo"/>
            </a></p>
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Running Jobs on Cirrus</a><ul>
<li><a class="reference internal" href="#using-pbs-pro">Using PBS Pro</a><ul>
<li><a class="reference internal" href="#the-qsub-command">The qsub command</a></li>
<li><a class="reference internal" href="#the-qstat-command">The qstat command</a></li>
<li><a class="reference internal" href="#the-qdel-command">The qdel command</a></li>
</ul>
</li>
<li><a class="reference internal" href="#queue-limits">Queue Limits</a></li>
<li><a class="reference internal" href="#output-from-pbs-jobs">Output from PBS jobs</a></li>
<li><a class="reference internal" href="#running-parallel-jobs">Running Parallel Jobs</a><ul>
<li><a class="reference internal" href="#exclusive-node-access">Exclusive Node Access</a></li>
</ul>
</li>
<li><a class="reference internal" href="#running-mpi-parallel-jobs">Running MPI parallel jobs</a><ul>
<li><a class="reference internal" href="#sgi-mpt-message-passing-toolkit">SGI MPT (Message Passing Toolkit)</a><ul>
<li><a class="reference internal" href="#sgi-mpt-parallel-launcher-mpiexec-mpt">SGI MPT: parallel launcher <code class="docutils literal"><span class="pre">mpiexec_mpt</span></code></a></li>
<li><a class="reference internal" href="#sgi-mpt-interactive-mpi-using-mpirun">SGI MPT: interactive MPI using <code class="docutils literal"><span class="pre">mpirun</span></code></a></li>
<li><a class="reference internal" href="#sgi-mpt-running-hybrid-mpi-openmp-applications">SGI MPT: running hybrid MPI/OpenMP applications</a></li>
</ul>
</li>
<li><a class="reference internal" href="#intel-mpi">Intel MPI</a><ul>
<li><a class="reference internal" href="#intel-mpi-parallel-job-launcher-mpirun">Intel MPI: parallel job launcher <code class="docutils literal"><span class="pre">mpirun</span></code></a></li>
<li><a class="reference internal" href="#intel-mpi-running-hybrid-mpi-openmp-applications">Intel MPI: running hybrid MPI/OpenMP applications</a></li>
<li><a class="reference internal" href="#intel-mpi-mpi-io-setup">Intel MPI: MPI-IO setup</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#example-parallel-mpi-job-submission-scripts">Example parallel MPI job submission scripts</a><ul>
<li><a class="reference internal" href="#example-sgi-mpt-job-submission-script-for-mpi-parallel-job">Example: SGI MPT job submission script for MPI parallel job</a></li>
<li><a class="reference internal" href="#example-sgi-mpt-job-submission-script-for-mpi-openmp-mixed-mode-parallel-job">Example: SGI MPT job submission script for MPI+OpenMP (mixed mode) parallel job</a></li>
<li><a class="reference internal" href="#example-job-submission-script-for-parallel-non-mpi-based-jobs">Example: job submission script for parallel non-MPI based jobs</a></li>
</ul>
</li>
<li><a class="reference internal" href="#serial-jobs">Serial Jobs</a></li>
<li><a class="reference internal" href="#job-arrays">Job arrays</a><ul>
<li><a class="reference internal" href="#job-script-for-a-job-array">Job script for a job array</a></li>
<li><a class="reference internal" href="#starting-a-job-array">Starting a job array</a></li>
<li><a class="reference internal" href="#querying-a-job-array">Querying a job array</a></li>
</ul>
</li>
<li><a class="reference internal" href="#interactive-jobs">Interactive Jobs</a></li>
<li><a class="reference internal" href="#reservations">Reservations</a><ul>
<li><a class="reference internal" href="#requesting-reservations">Requesting reservations</a></li>
<li><a class="reference internal" href="#checking-the-status-of-your-reservation">Checking the status of your reservation</a></li>
<li><a class="reference internal" href="#submitting-jobs-to-a-reservation">Submitting jobs to a reservation</a></li>
<li><a class="reference internal" href="#reservations-for-all-project-users">Reservations for all project users</a></li>
<li><a class="reference internal" href="#deleting-a-reservation">Deleting a reservation</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="development.html" title="previous chapter">Application Development Environment</a></li>
      <li>Next: <a href="singularity.html" title="next chapter">Singularity Containers</a></li>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/user-guide/batch.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2016, EPCC.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
      |
      <a href="../_sources/user-guide/batch.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>